{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f815ff5",
   "metadata": {},
   "source": [
    "# Week 5 - 주간학습정리 - NLP\n",
    "\n",
    "## 깨달은 내용\n",
    "* 신규용어\n",
    "* subword-level tokenization의 효용\n",
    "* Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5323e24a",
   "metadata": {},
   "source": [
    "## 신규 용어\n",
    "* 정규화(normalization) \n",
    "    * 표현 방법이 다른 단어들을 통합하여 같은 단어로 만듬\n",
    "    * 정규화 방법 : 어간 추출(stemming)과 표제어 추출(lemmatization) \n",
    "* 토큰화 (tokeniztion)\n",
    "    * 주어진 주어진 text를 token이라 불리는 단위로 분리\n",
    "    * 토큰(token)\n",
    "        * 자연어 처리모델이 각 타임스텝에서 단어로 다루는 단위\n",
    "        * 모든 토큰은 사전에 등록되어 있음\n",
    "        * 모든 토큰은 사전에 등록되어 있는 위치 즉, 인덱스로 표현 가능함\n",
    "    * 방법 ( 대표적인 3가지 )\n",
    "        * word-level toknenization\n",
    "        * character-level tokenization\n",
    "        * subword-level tokenization    \n",
    "* BPE Encoding\n",
    "    * BPE 기반의 subword-level tokenization\n",
    "* 형태소(morphs)\n",
    "    * 의미를 가진 가장 작은 단위\n",
    "    * 한국어는 조사, 어미 등을 붙여 말을 만들기 대문에 어절보다는 형태소를 기준으로 토큰화 진행\n",
    "    * 영어는 띄어쓰기 단위, '어절'을 기준으로 토큰화 진행\n",
    "    * 형태소 종류로 어간(stem), 접사(affix) 가 있음\n",
    "        * 어간(stem) : 단어의 의미를 담고 있는 단어의 핵심 부분\n",
    "        * 접사(affix) : 단어에 추가적인 의미를 주는 부분\n",
    "        * cats = cat(어간)-s(접사)\n",
    "* 품사(pos)\n",
    "    * POS : Part Of Speech    \n",
    "* 표제어(lemma)\n",
    "    * lemmatization \n",
    "* 어간 추출(stemming)\n",
    "* 불용어(stopword)\n",
    "* OOV(Out-of-vocabulary)\n",
    "    * 사전에 등록되어 있지 않은 토큰\n",
    "    * OOV는 \"Unknown\"토큰으로 처리\n",
    "    * \"Unknown\" token도 학습 대상이되고 학습될 수 있지만, 여러가지 단어가 혼재된 토큰이라 학습 결과는 무의미\n",
    "    * 현대 LLM에서는 subword-level의 tokenization을 사용하기 때문에 OOV 가 거의 발생하지 않음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1333e6cf",
   "metadata": {},
   "source": [
    "## subword-level tokenization의 효용\n",
    "* 토큰나이저의 목적\n",
    "    * 토큰나이저는 단순한 text 자르기가 아님\n",
    "    * 언어모델이 효율적으로 처리할 수 있도록 적절하게 나누는 것이 목적임\n",
    "        * 토큰을 너무 잘게 나누면\n",
    "            * 문장을 조립하는데 너무 긴 시퀀스가 필요해서 비효율적\n",
    "            * 연산과 메모리 많이 필요\n",
    "        * 토큰을 너무 크게 나누면 \n",
    "            * 사전의 사이즈가 커져야 해서 비효율 ( 토큰의 임베딩 벡터가 증가되어 학습파라미터 증가되어 비효율 )\n",
    "            * 사전 사이즈를 적정하게 유지하고 Unknown 토큰으로 처리해야 함 ( 처리불가능한 토큰이 증가되어 비효율 )\n",
    "    * 분리된 토큰은 모델이 사용하는데, 모델 입장에서 바라본 토큰은 다음과 같음\n",
    "        * 토큰은 더이상 쪼갤 수 없는 최소 단위\n",
    "        * 모든 입력된 문장은 토큰의 시퀀스로 변경되어 모델에게 입력됨\n",
    "        * 토큰은 모델이 이해하고 말 할 수 있는 모든 것임 ( 토큰 말고는 나는 알지 못함 )\n",
    "        * 모든 토큰은 사전에 등록되어 있고 각 토큰은 임베딩 벡터로 표현됨  \n",
    "        * 모든 토큰은 임베딩 스페이스의 한점임( 임베딩 스페이스는 768차원 혹은 1024차원 등의 고차원 공간 )\n",
    "* 토크나이저 요구사항\n",
    "    * 모든 텍스트를 토큰 혹은 토큰의 시퀀스로 표현할 수 있어야 함\n",
    "    * 수많은 사람이름, 지명이름, 신조어 등을 유의미한 토큰으로 처리할 수 있어야 함\n",
    "    * unknown token이 없어야 함\n",
    "    * 사전의 크기가 커지면 성능이 떨어지기 때문에 적절한 사전의 크기가 필요. ( 50_000 개 이하 )\n",
    "* 이러한 목적과 요구사항에 부합하는 방식이 subword-levle tokenization임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becd216b",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "* 단어를 dense vector로 표현하는 word embedding의 대표적 방법론\n",
    "* 단어의 vector 표현 방법\n",
    "    * one-hot vector ecoding\n",
    "        * 단어를 categorical variable로 encoding한 벡터로 표현\n",
    "        * sparse representation (대부분의 element가 zero)\n",
    "    * word embedding\n",
    "        * 단어의 의미를 여러 차원에 0이 아닌 element의 vector로 표현\n",
    "        * dense vector ( distributed vector )\n",
    "        * 유사한 의미의 단어는 유사도가 높은 위치에 있게 됨\n",
    "        * 토큰을 고차원의 embedding space의 한 점으로 표현\n",
    "* Word2Vec 학습 방법\n",
    "    * 분포 가설(Distributional Hypothesis)에 기반\n",
    "        * \"You shall know a word by the company it keeps\" -[J.R. Firth 1957](https://ko.wikipedia.org/wiki/%EC%A1%B4_%EB%A3%A8%ED%8D%BC%ED%8A%B8_%ED%8D%BC%EC%8A%A4)\n",
    "        * 퍼스는 '상황적 맥락'이라는 개념으로 의미의 맥락 의존적 본성에 주목을 이끌어낸 것으로 유명(위키백과)\n",
    "    * CBoW ( Continuos Bag of Words )\n",
    "        * 주변 단어(context)로 중심 단어(target) 예측\n",
    "        * 학습 속도가 빠름 ( 중심단어당 1번 학습 )\n",
    "    * Skip-gram\n",
    "        * 중심 단어로 주변 단어들 예측\n",
    "        * 주어진 windows 크기 이내만큼 떨어진 주변단어를 예측 ( skip의 의미 )\n",
    "        * CBoW에 비해 학습 속도 느림 ( 중심단어당 windows 수 * 2 만큼 학습 )\n",
    "        * two layer 선형변환으로 구현\n",
    "        * hidden layer에는 activation funtion을 적용하지 않음\n",
    "        * input의 사이즈는 사전에 등록된 단어의 수와 같음\n",
    "        * output의 사이즈는 사전에 등록된 단어의 수와 같음(모든 단어의 확률를 출력 함)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "boostcamp (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
