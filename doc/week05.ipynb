{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f815ff5",
   "metadata": {},
   "source": [
    "# Week 5 - 주간학습정리 - NLP\n",
    "\n",
    "## 깨달은 내용\n",
    "* 신규용어\n",
    "* subword-level tokenization의 효용\n",
    "* Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5323e24a",
   "metadata": {},
   "source": [
    "## 신규 용어\n",
    "* 정규화(normalization) \n",
    "    * 표현 방법이 다른 단어들을 통합하여 같은 단어로 만듬\n",
    "    * 정규화 방법 : 어간 추출(stemming)과 표제어 추출(lemmatization) \n",
    "* 토큰화 (tokeniztion)\n",
    "    * 주어진 주어진 text를 token이라 불리는 단위로 분리\n",
    "    * 토큰(token)\n",
    "        * 자연어 처리모델이 각 타임스텝에서 단어로 다루는 단위\n",
    "        * 모든 토큰은 사전에 등록되어 있음\n",
    "        * 모든 토큰은 사전에 등록되어 있는 위치 즉, 인덱스로 표현 가능함\n",
    "    * 방법 ( 대표적인 3가지 )\n",
    "        * word-level toknenization\n",
    "        * character-level tokenization\n",
    "        * subword-level tokenization    \n",
    "* BPE Encoding\n",
    "    * BPE 기반의 subword-level tokenization\n",
    "* 형태소(morphs)\n",
    "    * 의미를 가진 가장 작은 단위\n",
    "    * 한국어는 조사, 어미 등을 붙여 말을 만들기 대문에 어절보다는 형태소를 기준으로 토큰화 진행\n",
    "    * 영어는 띄어쓰기 단위, '어절'을 기준으로 토큰화 진행\n",
    "    * 형태소 종류로 어간(stem), 접사(affix) 가 있음\n",
    "        * 어간(stem) : 단어의 의미를 담고 있는 단어의 핵심 부분\n",
    "        * 접사(affix) : 단어에 추가적인 의미를 주는 부분\n",
    "        * cats = cat(어간)-s(접사)\n",
    "* 품사(pos)\n",
    "    * POS : Part Of Speech    \n",
    "* 표제어(lemma)\n",
    "    * lemmatization \n",
    "* 어간 추출(stemming)\n",
    "* 불용어(stopword)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1333e6cf",
   "metadata": {},
   "source": [
    "## subword-level tokenization의 효용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becd216b",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "* 단어를 dense vector로 표현하는 word embedding의 대표적 방법론\n",
    "* 단어의 vector 표현 방법\n",
    "    * one-hot vector ecoding\n",
    "        * 단어를 categorical variable로 encoding한 벡터로 표현\n",
    "        * sparse representation (대부분의 element가 zero)\n",
    "    * word embedding\n",
    "        * 단어의 의미를 여러 차원에 0이 아닌 element의 vector로 표현\n",
    "        * dense vector ( distributed vector )\n",
    "        * 유사한 의미의 단어는 유사도가 높은 위치에 있게 됨\n",
    "* Word2Vec 학습 방법\n",
    "    * 분포 가설(Distributional Hypothesis)에 기반\n",
    "        * \"You shall know a word by the company it keeps\" -[J.R. Firth 1957](https://ko.wikipedia.org/wiki/%EC%A1%B4_%EB%A3%A8%ED%8D%BC%ED%8A%B8_%ED%8D%BC%EC%8A%A4)\n",
    "        * 퍼스는 '상황적 맥락'이라는 개념으로 의미의 맥락 의존적 본성에 주목을 이끌어낸 것으로 유명(위키백과)\n",
    "    * CBoW ( Continuos Bag of Words )\n",
    "        * 주변 단어(context)로 중심 단어(target) 예측\n",
    "        * 학습 속도가 빠름 ( 중심단어당 1번 학습 )\n",
    "    * Skip-gram\n",
    "        * 중심 단어로 주변 단어들 예측\n",
    "        * 주어진 windows 크기 이내만큼 떨어진 주변단어를 예측 ( skip의 의미 )\n",
    "        * CBoW에 비해 학습 속도 느림 ( 중심단어당 windows 수 * 2 만큼 학습 )\n",
    "        * two layer 선형변환으로 구현\n",
    "        * hidden layer에는 activation funtion을 적용하지 않음\n",
    "        * input의 사이즈는 사전에 등록된 단어의 수와 같음\n",
    "        * output의 사이즈는 사전에 등록된 단어의 수와 같음(모든 단어의 확률를 출력 함)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551d2b23",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "boostcamp (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
