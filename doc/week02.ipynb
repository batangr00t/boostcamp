{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59159794",
   "metadata": {},
   "source": [
    "# Week 2 - 주간학습정리 - AI Math\n",
    "\n",
    "## 깨달은 내용\n",
    "* 혼란스러운 용어 정리\n",
    "* pseudo-inverse ( Moore-Penrose inverse ) - left, right\n",
    "* Ridge/Lasso regression\n",
    "* eigendecomposition\n",
    "* PCA(Principal Component Analysis)\n",
    "* SVD(Singular value Decomposition)\n",
    "* 다중선형회귀 구조 분석\n",
    "* 대수의 법칙\n",
    "* 확률분포의 거리 (쿨백-라이블러 발산, Kullback-Leibler divergence)\n",
    "\n",
    "## 혼란스러운 용어 정리\n",
    "* 차원 (dimention, ndim)\n",
    "    * 선형대수에서 사용하는 차원의 의미 : vector space의 base element의 원소 갯수\n",
    "    * phtion, numpy, pytorch에서 사용하는 차원의 의미\n",
    "        * ndim : 차원의 갯수 = 중첩배열의 중첩 횟수\n",
    "        * 여러 함수에서 사용하는 dim=0, dim=1 같은 코드의 의미는 가장 바깥 배열을 0으로 하여 안쪽 배열로 들어갈 수록 1씩 증가하여 index를 부여한 것\n",
    "    * 비교 예시\n",
    "        * $A = [[1,3], [2,6]]$\n",
    "        * 수학에서 A column space 의 차원은 1임 : base가 {$[1,3]$}으로 원소가 하나이기 때문\n",
    "        * A.ndim=2 임 : 배열이 두번 중첩되어 있기 때문\n",
    "* inner prodoct : 라이브러리, 수학 차이\n",
    "    * vector : 두 벡터의 내적을 구하는 것으로 라이브러리, 수학에서 동일한 개념\n",
    "        * np.inner(a, b) = np.sum(a*b) : 각 성분별 곱의 합\n",
    "    * matrix : 서로 다름\n",
    "        * 라이브러리 : 마지막 차원의 벡터 내적을 계산, 결과는 행렬\n",
    "        * 수학 : 프로베니우스 내적이 대표적인 행렬간 내적, 결과는 스칼라\n",
    "        * 프로베니우스 내적 : $tr(XY^T)$ = X, Y의 element-wise 곱의 합\n",
    "\n",
    "## pseudo-inverse ( Moore-Penrose inverse ) - left, right\n",
    "* m, n이 달라 역행렬을 계산할 수 없을 때 유사 역행렬을 이용할 수 있음\n",
    "    * left inverse\n",
    "        * $A^+ = (A^TA)^{-1}A^T$\n",
    "        * 답이 없는 상황에서 유사해 찾기\n",
    "        * 열공간 Col(A)에서 b와 가장 가까운 점 찾기\n",
    "    * right inverse \n",
    "        * $A^+ = A^T(AA^T)^{-1}$\n",
    "        * 답이 무수히 많은 상황에서 경제적인 답 찾기\n",
    "        * $Ax=b$를 만족하는 무수히 많은 해 중 Row(A)에 속하는 해 찾기\n",
    "* 조건\n",
    "    * 위 계산식은 row full rank이거나, column full rank일 때 사용하는 방식\n",
    "    * rank가 row나 column 보다도 적은 경우는 SVD 이용하여 구할 수 있음\n",
    "\n",
    "## Ridge/Lasso regression\n",
    "* 다중회귀 모형에서 과적합이 발생할 때 일반화 성능을 높이기 위해 개선한 방법\n",
    "* Ridge 회귀\n",
    "    * 손실에 파라미터 제곱을 패널티 항으로 더해준다\n",
    "    * 패널티 항이 $\\lambda \\sum_{i=1}^p\\beta^2$ 이기 때문에 L2 Regression 이라고도 한다\n",
    "    * 제약조건 영역을 그리면 원형으로 나타나기 때문에 일부 파라미터가 0에 가까워 질수 있다\n",
    "* Lasso 회귀\n",
    "    * 손실에 파라미터 절대값을 패널티 항으로 더해준다\n",
    "    * 패널티 항이 $\\lambda \\sum_{i=1}^p\\lvert\\beta\\rvert$ 이기 때문에 L1 Regression 이라고도 한다\n",
    "    * 제약조건에 경계를 그리면 마름모꼴로 나타나기 때문에 $\\beta$의 제약조건에 맞는 값을 찾을 대 모서리에서 찾을 확률이 있어서, 그 때는 일부 파라미터 값이 0이기 때문에 파라미터 selection 효과가 있다\n",
    "\n",
    "## matrix decomposition\n",
    "### QR decomposition\n",
    "* 행렬 A는 아래와 같이 분해될 수 있음\n",
    "    * $A = QR$\n",
    "    * $Q: m \\times m \\text{ orthogonal, unitary matrix}$\n",
    "    * $R: m \\times n \\text{ upper triangular matrix}$ \n",
    "* 조건 \n",
    "    * column full rank,  rank(A) = m\n",
    "* 용도\n",
    "    * $Ax=b$ 방정식을 푸는데 편리함 : $ Rx = Q^{T}b $ 로 변경하여 쉽게 품\n",
    "    * eigen value, eigen vector 구하는데 활용할 수 있음\n",
    "        $$\n",
    "        \\begin{align*} \n",
    "        & A_1 = Q_1 R_1 \\\\\n",
    "        \\\\\n",
    "        & A_2 = R_1 Q_1 \\\\\n",
    "        \\rightarrow & A_2 = (I) R_1 Q_1 = (Q_1^TQ_1) R_1 Q_1 = Q_1^T (Q_1 R_1) Q_1 = Q_1^T A_1 Q_1 \\\\\n",
    "        \\rightarrow & A_1 \\text{ is similar to } A_2 \\\\\n",
    "        \\rightarrow & A_1, A_2\\text { 는 engenvalue가 같다} \\\\\n",
    "        \\\\\n",
    "        & A_{k+1} = R_k Q_k \\text{ 로 두면} \\\\\n",
    "        \\rightarrow & A_{k+1} = (Q_k^TQ_k) R_k Q_k = Q_k^T (Q_k R_k) Q_k = Q_k^T A_k Q_k \\\\\n",
    "        \\rightarrow & A_{k+1} \\text{ is similar to }  A_k\n",
    "        \\end{align*} \n",
    "        $$\n",
    "    * 결국 $A_1, A_2, A_3, ... A_n$ 은 모두 같은 eigen value를 갖는다\n",
    "\n",
    "### eigen decomposition\n",
    "* 행렬 A는 아래와 같이 분해할 수 있음\n",
    "    * $A = V\\Lambda V^{-1}$\n",
    "    * $V$ : 고유벡터를 열벡터로 한 행렬\n",
    "    * $\\Lambda$ : 고유값이 대각성분인 행렬\n",
    "* 고유값 분해의 조건\n",
    "    * 행렬 A가 square matrix \n",
    "        * eigen value, eigen vector 정의가 $Av=\\lambda v$\n",
    "        * $\\Rightarrow$ 정의역과 공역에 속해있는 원소들의 차원이 동일해야 함\n",
    "        * $\\Rightarrow$ A가 nxn matrix\n",
    "    * A의 모든 고유벡터가 선형독립\n",
    "        * eigendecomposition이 성립하려면 $V^{-1}$가 존재해야 함\n",
    "        * $\\Leftrightarrow$ V가 full rank  \n",
    "        * $\\Leftrightarrow$ 모든 고유벡터가 선형독립\n",
    "* 고유값 분해 방법\n",
    "    * QR Decomposition 이용하는 방법\n",
    "    * Singular Value Decomposition 이용하는 방법\n",
    "* 용도\n",
    "    * $A^n$ 계산 : $A^n = V \\Lambda^n V^{-1}$\n",
    "    * PCA(Principal Component Analysis) : [PCA process](week02_PCA.ipynb)\n",
    "\n",
    "## Singular Value Decomposition(SVD)\n",
    "* 행렬 A는 아래와 같이 분해할 수 있음\n",
    "    * $A=U \\Sigma V^{T}$\n",
    "* 활용\n",
    "    * 인버스에 활용\n",
    "        * $ A^{+} = V\\Sigma^{+}U^{T} $ \n",
    "    * eigen decposition 구하는데 활용 ( 제한적 )\n",
    "\n",
    "## 다중선형회귀 구조 분석\n",
    "* 일반적인 선형회귀 구조\n",
    "    * 데이터 $ X=(m,n), y=(m,1) $가 주어짐\n",
    "    * 주어진 데이터는 X -> Linear function => activation function -> loss function 를 거처 scalar로 계산\n",
    "        * Linear function : $z = X\\beta + b, \\qquad z=(m,1), X=(m,n), \\beta=(n,1), b=(1,1)$\n",
    "            * 엄밀한 관점에서는 선형변환이 아니지만, $[X, 1]$ 형태로 해석할 수 있기 때문에 선형 변환이라고 인식함\n",
    "            * $[X, 1]$ column들의 선형결합(linear combination)을 계산하여 변환\n",
    "        * activation function : $\\hat{y} = \\sigma(z), \\qquad \\hat{y}=(m,1), z=(m,1)  $\n",
    "            * 비선형성을 가미하여 다양한 비선형 모델을 만들기 위해 추가\n",
    "            * activation 함수를 사용하지 않을 경우엔 activation function으로 identity() 함수를 사용한 것으로 간주할 수 있음\n",
    "            * 활성화 함수 종류 : step(), sigmoid(), tanh(), ReLU(), leakyReLU(), softmax()\n",
    "            * 모두 vector를 같은 shape의 vector로 변환하는 element-wise 함수\n",
    "        * loss function : $L = loss(y, \\hat{y}) \\qquad L=(1,1), y=(m,1), \\hat{y}=(m,1) $\n",
    "            * 예측값과 target의 차이를 측정하기 위해 사용, 이 손실을 최소화 하는 방향으로 최적화 진행\n",
    "            * 손실 함수 종류 : MAE, MSE, RMSE, BCE, CCE\n",
    "            * 모두 vector를 스칼라로 변환하는 함수\n",
    "    * 학습한 결과를 활용할 때는\n",
    "        * 취득한 데이터가 $x$일 때 $x\\beta +b$로 결과 예측, $x=(1,n), \\beta=(n,1), b=(1,1)$\n",
    "    * 이를 시각적으로 표현하면 \n",
    "        * training\n",
    "            * 주어진 데이터 $X, y$로 training과정을 거쳐 $\\beta, b$ 구하기\n",
    "            * loss function의 결과가 최소화 되도록 $\\beta, b$를 update 시킴\n",
    "            * $X, y$: 상수,  $\\beta, b$: 변수\n",
    "        $$\n",
    "        \\begin{bmatrix}\n",
    "            \\cdots & (x^{(1)})^T & \\cdots\\\\\n",
    "            \\cdots & (x^{(2)})^T & \\cdots\\\\\n",
    "            & \\vdots & \\\\\n",
    "            & \\vdots & \\\\\n",
    "            & \\vdots & \\\\\n",
    "            & \\vdots & \\\\\n",
    "            \\cdots & (x^{(m)})^T & \\cdots\\\\\n",
    "        \\end{bmatrix} @\n",
    "        \\begin{bmatrix}\n",
    "            \\beta_1 \\\\\n",
    "            \\beta_2 \\\\\n",
    "            \\vdots \\\\\n",
    "            \\beta_n \\\\\n",
    "        \\end{bmatrix} +\n",
    "         \\begin{bmatrix}\n",
    "            b \n",
    "        \\end{bmatrix} =\n",
    "        \\begin{bmatrix}\n",
    "            z_1 \\\\\n",
    "            z_2 \\\\\n",
    "            \\vdots \\\\\n",
    "            \\vdots \\\\\n",
    "            \\vdots \\\\\n",
    "            \\vdots \\\\\n",
    "            z_m \\\\\n",
    "        \\end{bmatrix} \\xrightarrow{\\sigma}\n",
    "        \\begin{bmatrix}\n",
    "            \\hat{y}_1 \\\\\n",
    "            \\hat{y}_2 \\\\\n",
    "            \\vdots \\\\\n",
    "            \\vdots \\\\\n",
    "            \\vdots \\\\\n",
    "            \\vdots \\\\\n",
    "            \\hat{y}_m \\\\\n",
    "        \\end{bmatrix} \\approx\n",
    "        \\begin{bmatrix}\n",
    "            y_1 \\\\\n",
    "            y_2 \\\\\n",
    "            \\vdots \\\\\n",
    "            \\vdots \\\\\n",
    "            \\vdots \\\\\n",
    "            \\vdots \\\\\n",
    "            y_m \\\\\n",
    "        \\end{bmatrix}\n",
    "        $$\n",
    "        * 추론\n",
    "            * 주어진 파라미터 $\\beta, b$로 임의의 $x$에 대한 결과 구하기\n",
    "            * $\\sigma(x\\beta +b)$로 예측하고 결과 $\\hat{y}$은 real $y$에 근접할 것이라고 기대\n",
    "            * $\\beta, b$: 상수, $x$: 변수\n",
    "        $$\n",
    "        \\begin{bmatrix}\n",
    "         x_1 x_2 \\cdots x_n\n",
    "        \\end{bmatrix} @\n",
    "        \\begin{bmatrix}\n",
    "            \\beta_1 \\\\\n",
    "            \\beta_2 \\\\\n",
    "            \\vdots \\\\\n",
    "            \\beta_n \\\\\n",
    "        \\end{bmatrix} +\n",
    "        \\begin{bmatrix}\n",
    "            b \n",
    "        \\end{bmatrix} =\n",
    "        \\begin{bmatrix}\n",
    "            z \n",
    "        \\end{bmatrix} \\xrightarrow{\\sigma} \n",
    "        \\begin{bmatrix}\n",
    "            \\hat{y}\n",
    "        \\end{bmatrix} \\approx\n",
    "        \\begin{bmatrix}\n",
    "            y\n",
    "        \\end{bmatrix}\n",
    "        $$        \n",
    "* 최적화 방법 : 경사하강법(Gradient descent)\n",
    "    * 손실함수가 줄어드는 방향으로 $\\beta, b$를 수정을 반복 수행\n",
    "    * 손실함수에 대한 $\\beta, b$의 편미분이 사용됨\n",
    "        $$\n",
    "        \\begin{align*} \n",
    "        & \\frac{\\partial L}{\\partial \\beta} = \\frac{\\partial L}{\\partial \\hat{y}} \n",
    "                                            \\frac{\\partial \\hat{y}}{\\partial z}\n",
    "                                            \\frac{\\partial z}{\\partial \\beta},  \n",
    "        \\qquad \\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial \\hat{y}} \n",
    "                                            \\frac{\\partial \\hat{y}}{\\partial z}\n",
    "                                            \\frac{\\partial z}{\\partial b}  \\\\\n",
    "        \\end{align*}\n",
    "        $$\n",
    "        * $\\frac{\\partial L}{\\partial \\hat{y}}$ : 손실함수에 따라 달라짐\n",
    "        * $\\frac{\\partial \\hat{y}}{\\partial z}$ : 액티베이션 함수에 따라 달라짐\n",
    "        * $\\frac{\\partial z}{\\partial \\beta}, \\frac{\\partial z}{\\partial b}$: 다중선형회귀는 항상 동일\n",
    "    * 각 편미분을 구해보면\n",
    "        * $\\frac{\\partial L}{\\partial \\hat{y}}$ 은 손실함수별로 달라지나 형태는 동일함\n",
    "            $$\n",
    "            \\frac{\\partial L}{\\partial \\hat{y}} =                 \n",
    "            \\begin{bmatrix}\n",
    "                \\frac{\\partial L}{\\partial \\hat{y}_1} \\\\\n",
    "                \\frac{\\partial L}{\\partial \\hat{y}_2} \\\\\n",
    "                \\vdots \\\\\n",
    "                \\frac{\\partial L}{\\partial \\hat{y}_m} \\\\\n",
    "            \\end{bmatrix}, \\qquad \\frac{\\partial L}{\\partial \\hat{y}_i} \\text{ 는 i번째 예측값이 변할 때 Loss의 변화율을 의미 }\n",
    "            $$\n",
    "        * $\\frac{\\partial \\hat{y}}{\\partial z}$ 은 activaion function 별로 달라지나 형태는 동일함\n",
    "            $$\n",
    "            \\frac{\\partial \\hat{y}}{\\partial z} =\n",
    "            \\begin{bmatrix}\n",
    "                \\frac{\\partial \\hat{y}_1}{\\partial z_1} & 0 & \\cdots & 0\\\\\n",
    "                0 & \\frac{\\partial \\hat{y}_2}{\\partial z_2}  & \\cdots & 0\\\\\n",
    "                \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                0 & 0 & \\cdots & \\frac{\\partial \\hat{y}_m}{\\partial z_m} \\\\\n",
    "            \\end{bmatrix}, \\qquad \\frac{\\partial \\hat{y}_i}{\\partial z_i}  \\text{ 는 i번째 선형변환 값이 변할 때 i번째 예측값의 변화율을 의미 }\n",
    "            $$\n",
    "            * $\\frac{\\partial \\hat{y}_i}{\\partial z_j} = 0, \\text{ when } i \\neq j$ \\\n",
    "            activation funtion은 element-wise 함수이기 때문에 위치가 다른 변수끼리 영향을 주지 않음\n",
    "            * 코드로 실행할 때는 $\\frac{\\partial \\hat{y}}{\\partial z}$ 를 대각선 성분만으로 vector로 생성하고 element-wise product로 처리\n",
    "            ```python\n",
    "            # error = dL/dy_hat, (m,1) shape\n",
    "            dy_hat_dz = - ( y - y_hat ) # (m,1) vector로 처리\n",
    "            dL_dz = error * dy_hat_dz   # *는 element-wise product \n",
    "            ```\n",
    "        * $\\frac{\\partial z}{\\partial \\beta}, \\frac{\\partial z}{\\partial b}$ 는 다중선형회귀에서는 항상 동일함\n",
    "            * $z = X\\beta + b$\n",
    "            $$\n",
    "            \\begin{bmatrix}\n",
    "                x_{11} & x_{12} & \\cdots & x_{1n} \\\\\n",
    "                x_{21} & x_{22} & \\cdots & x_{2n} \\\\\n",
    "                \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "                \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "                \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "                \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "                x_{m1} & x_{m2} & \\cdots & x_{mn} \\\\\n",
    "            \\end{bmatrix} @\n",
    "            \\begin{bmatrix}\n",
    "                \\beta_1 \\\\\n",
    "                \\beta_2 \\\\\n",
    "                \\vdots \\\\\n",
    "                \\beta_n \\\\\n",
    "            \\end{bmatrix} +\n",
    "            \\begin{bmatrix}\n",
    "                b \n",
    "            \\end{bmatrix} =\n",
    "            \\begin{bmatrix}\n",
    "                z_1 \\\\\n",
    "                z_2 \\\\\n",
    "                \\vdots \\\\\n",
    "                \\vdots \\\\\n",
    "                \\vdots \\\\\n",
    "                \\vdots \\\\\n",
    "                z_m \\\\\n",
    "            \\end{bmatrix} \n",
    "            $$\n",
    "            * 다음과 같은 형식\n",
    "            $$\n",
    "            \\begin{align*} \n",
    "            & \\frac{\\partial z}{\\partial \\beta} =\n",
    "            \\begin{bmatrix}\n",
    "                \\frac{\\partial z_1}{\\partial \\beta_1} & \\frac{\\partial z_2}{\\partial \\beta_1} & \\cdots &\\frac{\\partial z_m}{\\partial \\beta_1} \\\\\n",
    "                \\frac{\\partial z_1}{\\partial \\beta_2} & \\frac{\\partial z_2}{\\partial \\beta_2} & \\cdots & \\frac{\\partial z_m}{\\partial \\beta_2} \\\\\n",
    "                \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "                \\frac{\\partial z_1}{\\partial \\beta_n} & \\frac{\\partial z_2}{\\partial \\beta_n} & \\cdots & \\frac{\\partial z_m}{\\partial \\beta_n}\\\\\n",
    "            \\end{bmatrix} = X^T,\\quad & \\frac{\\partial z_i}{\\partial \\beta_j} \n",
    "            \\text{ 는 j번째 beta가 변할때 zi가 얼마만큼 변하는지 의미 } \\\\\n",
    "            & \\frac{\\partial z}{\\partial b} = \n",
    "            \\begin{bmatrix}\n",
    "                \\frac{\\partial z_1}{\\partial b}\\\\\n",
    "                \\frac{\\partial z_2}{\\partial b}\\\\\n",
    "                \\vdots \\\\\n",
    "                \\frac{\\partial z_m}{\\partial b}\\\\\n",
    "            \\end{bmatrix} =                 \n",
    "            \\begin{bmatrix}\n",
    "                1\\\\ 1\\\\ \\vdots \\\\  1\\\\\n",
    "            \\end{bmatrix}, \\quad & \\frac{\\partial z_i}{\\partial b} \n",
    "            \\text{ 는 b가 변할 때 zi가 얼마만큼 변하는지 의미 } \\\\\n",
    "            \\end{align*}\n",
    "            $$\n",
    "            * python 코드로 보면 \n",
    "                ```python\n",
    "                # error = dL/dz (m, 1) shape\n",
    "                beta_grad = np.transpose(X) @ error\n",
    "                b_grad = np.transpose(np.ones(m,1)) @ error = np.sum(error)\n",
    "                ```\n",
    "* 손실함수로 MSE를 사용할 때 다중선형회귀 구조는 아래와 같음\n",
    "    * 위에서 정리한 일반적인 다중선형회의 구조에 대입하여 해석하면 아래와 같음\n",
    "        * Linear function : 변화 없이 동일,  $z = X\\beta + b$\n",
    "        * activation function : 사용하지 않음으로 $\\hat{y} = z$\n",
    "        * loss function : $MSE = \\frac{1}{m}\\lVert y - \\hat{y} \\rVert^2 $\n",
    "\n",
    "    * 손실함수와 그레디언트\n",
    "        * 벡터 미분 기본 성질\n",
    "            $$\n",
    "            \\begin{align*} \n",
    "            & \\nabla_\\beta(a^T\\beta) = a, \\nabla_\\beta(\\beta^Ta) = a \\\\\n",
    "            & \\nabla_x(x^Tx) = 2x\\\\\n",
    "            & \\nabla_\\beta(\\beta^TA\\beta) = (A+A^T)\\beta, \\quad 2A\\beta \\text{(when A is symetric)} \\\\\n",
    "            & \\nabla_\\beta(\\beta^T A^TA\\beta) = 2A^TA\\beta, \\quad \\because A^TA \\text{ is symetric} \\\\\n",
    "            \\end{align*}\n",
    "            $$\n",
    "        * 손실함수\n",
    "            $$\n",
    "            \\begin{align*} \n",
    "            MSE & = \\frac{1}{m}\\lVert y - \\hat{y} \\rVert^2 \\\\\n",
    "            & = \\frac{1}{m} (y - \\hat{y})^T(y -\\hat{y}) \\\\\n",
    "            & = \\frac{1}{m}(y^Ty - y^T\\hat{y} - \\hat{y}^Ty + \\hat{y}^T\\hat{y}) \\\\\n",
    "            & = \\frac{1}{m}(y^Ty -2y^T\\hat{y} + \\hat{y}^T\\hat{y}) \\qquad (\\because \\hat{y}^Ty \\text{ is scalar, so } = y^T\\hat{y} )\n",
    "            \\end{align*}\n",
    "            $$\n",
    "        * 그레디언트 of MSE\n",
    "            $$\n",
    "            \\begin{align*} \n",
    "            \\nabla_{\\hat{y}} L \n",
    "            & = \\nabla_{\\hat{y}} \\big( \\frac{1}{m}\\lVert y - \\hat{y} \\rVert^2  \\big) \\\\\n",
    "            & = \\nabla_{\\hat{y}} \\big( \\frac{1}{m}(y^Ty -2y^T\\hat{y} + \\hat{y}^T\\hat{y}) \\big) \\qquad  (\\because\\text{ 위 손실함수에서 정리한 식})\\\\\n",
    "            & = \\frac{1}{m}(0 -2y + 2\\hat{y}), \\qquad (\\because \\text{위 벡터 미분 성질 이용 } ) \\\\\n",
    "            & = -\\frac{2}{m}(y - \\hat{y}) \\\\\n",
    "            \\\\\n",
    "            \\nabla_{z} \\hat{y} \n",
    "            & = ones(), \\text {(m,1) shape}\\\\\n",
    "            \\end{align*}\n",
    "            $$\n",
    "        * 최종 $\\nabla_\\beta L, \\nabla_b L$\n",
    "            $$\n",
    "            \\begin{align*} \n",
    "            \\nabla_\\beta L\n",
    "            & = \\frac{\\partial L}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial z} \\frac{\\partial z}{\\partial \\beta} \\\\\n",
    "            & = X^T @ ( ones() * (-\\frac{2}{m}(y - \\hat{y})) ) \\\\\n",
    "            & = -\\frac{2}{m} X^T @ (y - \\hat{y}) \\\\\n",
    "            \\\\\n",
    "            \\nabla_b L\n",
    "            & = \\frac{\\partial L}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial z} \\frac{\\partial z}{\\partial b} \\\\\n",
    "            & = ones()^T @ \\big( ones() * (-\\frac{2}{m}(y - \\hat{y}))\\big) \\\\\n",
    "            & = -\\frac{2}{m} ones()^T @ (y - \\hat{y}) \\\\\n",
    "            & = -\\frac{2}{m} sum(y - \\hat{y}) \\\\\n",
    "            & = -2mean(y - \\hat{y}) \\\\\n",
    "            \\end{align*}\n",
    "            $$ \n",
    "\n",
    "* SGD 대부분의 경우 전체 데이터 대신 batch_size만큼의 데이터로 GD 수행\n",
    "    * 특성\n",
    "        * SGD는 파라미터 update 시 전체 데이터를 한꺼번에 쓰지 않을 뿐 1 epoch 동안 전체 data를 사용한다\n",
    "        * SGD는 매번 다른 데이터를 선택하면서 그레디언트 벡터가 달라지기 때문에 매번 다른 골짜기로 내려간다. \\\n",
    "          이리저리 자주 방향을 바꾸며 탐색하는 꼴이며, 손실함수가 볼록이 이닌경우에도 효율적이다\n",
    "    * 유의 사항\n",
    "        * 학습률을 고정시키면 안되며, 스케줄러를 사용하는 것이 좋음\n",
    "          $$\n",
    "          \\lambda_t \\propto t^{-1} \n",
    "          \\implies \n",
    "          \\sum_{t}\\lambda_t = \\infty \\text{ and } \n",
    "          \\sum_{t}\\lambda_t^2 < \\infty \n",
    "          $$\n",
    "        * 로빈스-먼로(Robbins-Monro) 조건\n",
    "        * 간단한 2차 함수를 MSE 로 학습하는 것을 수행해 본 결과 로빈스-먼로 방식으로 할 경우 학습속도가 너무 느린 단점이 있었음\n",
    "        \n",
    "## 대수의 법칙\n",
    "* 대수의 법칙\n",
    "    * 반복적으로 독립추출 할 수 있을 때 기대값으로 거의 확실히 수렴\n",
    "    $$\n",
    "    \\frac{X_1+X_2+...+X_n}{n} \\implies E_{x\\sim p(x)}[X]\n",
    "    $$\n",
    "    * 기대값이 발산하는 경우는 적용되지 않음\n",
    "* 몬테카를로 sampling\n",
    "    * 확률변수가 이산형, 연속형 모두 적용됨\n",
    "    $$\n",
    "    E_{x\\sim p(x)}[f(x)] \\approx \\frac{1}{N} \\sum_{i=1}^{N} f(x^{(i)})\n",
    "    $$\n",
    "\n",
    "## 확률분포의 거리 (쿨백-라이블러 발산, Kullback-Leibler divergence)\n",
    "* P(x), Q(x) 확률분포 사이의 거리를 구함\n",
    "* 두 확률분포가 동일한 샘플공간에서 정의 된다고 하면\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    KL(P\\|Q) \n",
    "    & = \\sum_{x\\in \\chi}P(x)\\log\\left(\\frac{P(x)}{Q(x)}\\right) \\\\\n",
    "    & = \\int_{x}P(x)\\log\\left(\\frac{P(x)}{Q(x)}\\right)dx \\\\\n",
    "    \\\\\n",
    "    & \\text{* p(x)의 Q(x)에 대한 log 비율 개대값과 같음}\n",
    "    \\end{align*}\n",
    "    $$\n",
    "* 다음과 같이 해석할 수 있음\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    KL(P\\|Q) \n",
    "    & = - \\underbrace{E_{x\\sim p(x)}[logQ(x)]}_{교차엔트로피} \n",
    "      + \\underbrace{E_{x\\sim p(x)}[logP(x)]}_{엔트로피} \\\\\n",
    "    \\\\\n",
    "    & \\text{ p(x) = Q(x) 일때는 0이 됨}\n",
    "    \\end{align*}\n",
    "    $$\n",
    "* 분류문제에서 정답은 P, 모델예측을 Q라 할때 최대가능도 추정은 $KL(P\\|Q)$를 최소화하는 것과 같다"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
