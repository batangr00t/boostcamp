{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e247d6e",
   "metadata": {},
   "source": [
    "# PCA example \n",
    "* \"기계공학을 위한 머신러닝 - PCA\"강의를 듣고 이해한 후 주석을 추가하여 정리\n",
    "    * 자료 : [HTML](https://i-systems.github.io/teaching/ML/iNotes/07_Dim_Reduction.html), [PDF](https://drive.google.com/file/d/1ioQEkoQKOvFXsr5hqEYpszZn9IkIhR2h/view)\n",
    "    * 동영상 강의 : [youtube](https://www.youtube.com/playlist?list=PLGMtjo8jDX9ATrKsA4rQqQtCPuQc7dET6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf24b60",
   "metadata": {},
   "source": [
    "### 1. Pre-procession\n",
    "* Step 1: Given Data\n",
    "    * n개의 feature를 갖는 m개의 data X\n",
    "    * $x^{(i)}$는 n개의 feature를 갖는 데이터 한개를 의미하며, 모든 벡터는 개념적으로 column vector를 기본으로 함\n",
    "    * 계산할 때는 편의대로 row, column을 변경해 가며 계산 함\n",
    "    $$\n",
    "    x^{(i)} = \\begin{bmatrix} x^{(i)}_1 \\\\ \\vdots \\\\x^{(i)}_n \\end{bmatrix},\\qquad X = \\begin{bmatrix} \\cdots & (x^{(1)})^T & \\cdots\\\\\n",
    "    \\cdots & (x^{(2)})^T & \\cdots\\\\\n",
    "    & \\vdots & \\\\\n",
    "    \\cdots & (x^{(m)})^T & \\cdots\\\\\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "* Step 2: Shifting(Zero Mean)\n",
    "    * 평균이 zero가 되도록 하기 위해\n",
    "    * 평균 $\\mu$\n",
    "    $$\n",
    "    \\mu = \\frac{1}{m}\\sum\\limits_{i=1}^{m}x^{(i)}\n",
    "    $$\n",
    "    * 각 sample data별로 평균을 빼서 이동\n",
    "    $$\n",
    "    x^{(i)} \\leftarrow x^{(i)} - \\mu \\quad \\text{(zero mean)}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45eabf66",
   "metadata": {},
   "source": [
    "### 2. Maximize Variance\n",
    "* PCA의 목표는 m개 데이타를 n보다 차원이 적은 subspace로 projection 했을 때 variance()가 최대가 되는 subspace를 찾는 것임\n",
    "    * variance가 최대가 되는 것이 정보를 가장 많이 유지 하는 것이기 때문임\n",
    "    * 즉, projection 할 subspace의 othonomal한 base를 찾는 것임\n",
    "    * Ky Fan's Theorem 에 의해 Total Var = Var(첫 번째 축) + Var(두 번째 축) ... 이기 때문에\n",
    "    * projection 할 때 가장 variance가 크도록 하는 하나의 unit vector를 찾으면 됨\n",
    "* Step 1: Data Projection\n",
    "    * X가 $u$ 에 projection 한다고 가정\n",
    "    $$\n",
    "    X = \\begin{bmatrix} \\cdots & (x^{(1)})^T & \\cdots\\\\\n",
    "    \\cdots & (x^{(2)})^T & \\cdots\\\\\n",
    "    & \\vdots & \\\\\n",
    "    \\cdots & (x^{(m)})^T & \\cdots\\\\\n",
    "    \\end{bmatrix}, \\qquad\n",
    "    u = \\begin{bmatrix} u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_m \\end{bmatrix}\n",
    "    $$\n",
    "* Step 2: Maximizing the Variance of Projected Data\n",
    "    * $x^{(i)}$ vector를 단위 벡터 $u$에 projection 한 vector 크기: \n",
    "    $$\n",
    "    \\left\\Vert\\Pr_u(x^{(i)})\\right\\Vert= \\left\\Vert(u^T \\cdot x^{(i)})u\\right\\Vert = u^T \\cdot x^{(i)}\n",
    "    $$\n",
    "    * projection 된 data의 분산 $Var(\\Pr_u(X))$ 는 $x^{(i)}$평균이 0이기 때문에 아래와 같다\n",
    "    $$ \n",
    "    \\begin{align*}\n",
    "    & = \\frac{1}{m}\\sum\\limits_{i=1}^{m}\\left\\Vert\\Pr_u(x^{(i)})-\\bar{x^{(i)}}\\right\\Vert^2 \\\\\n",
    "    & = \\frac{1}{m}\\sum\\limits_{i=1}^{m}\\left\\Vert\\Pr_u(x^{(i)})-0\\right\\Vert^2 \\\\\n",
    "    & = \\frac{1}{m}\\sum\\limits_{i=1}^{m}\\left\\Vert\\Pr_u(x^{(i)})\\right\\Vert^2 \\\\\n",
    "    & = \\frac{1}{m}\\sum\\limits_{i=1}^{m}(u^T \\cdot x^{(i)})^2 \\\\\n",
    "    \\end{align*}\n",
    "    $$\n",
    "    * variance of projected data\n",
    "    $$\n",
    "    \\begin{align*} \n",
    "    & = \\frac{1}{m}\\sum\\limits_{i=1}^{m}\\big(u^Tx^{(i)}\\big)^2  = \\frac{1}{m}\\sum\\limits_{i=1}^{m}\\big( {x^{(i)}}^Tu\\big)^2 \\\\\n",
    "    & = \\frac{1}{m}\\sum\\limits_{i=1}^{m}\\big( {x^{(i)}}^Tu\\big)^T\\big( {x^{(i)}}^Tu\\big) = \\frac{1}{m}\\sum\\limits_{i=1}^{m}u^Tx^{(i)}{x^{(i)}}^Tu \\\\\n",
    "    & = u^T\\left(\\frac{1}{m}\\sum\\limits_{i=1}^{m}x^{(i)}{x^{(i)}}^T \\right) u \\\\\n",
    "    & =u^TSu \\qquad (S=\\frac{1}{m}X^T X: \\text{sample covariance matrix})\n",
    "    \\end{align*}\n",
    "    $$\n",
    "    * S 는 mxm matrix 임 ( m은 feature 갯수 )\n",
    "* Step 3. Optimization Formulation\n",
    "    * 분산 최대화 u 찾기 = 다음을 만족하는 u 찾기\n",
    "      $$\n",
    "      \\begin{align*} \\text{maximize} \\quad & u^TSu \\\\\n",
    "      \\text{subject to} \\quad\n",
    "      & u^Tu = 1\\end{align*}\n",
    "      $$\n",
    "\n",
    "* Step 4. Eigenvalue Analysis\n",
    "    * 라그장주 승부법에 의해 $Su = \\lambda u$\n",
    "    * u는 eigenvector, $\\lambda$는 eigenvalue\n",
    "    * S의 eigen value중 가장 큰 것을 찾으면 그 값이 최대의 분산이 되고, 대응되는 eigen vector가 첫번째 주성분이 됨\n",
    "    $$ \n",
    "    \\begin{align*} & u^TSu = u^T\\lambda u = \\lambda u^Tu = \\lambda \\\\\n",
    "    \\\\\n",
    "    & \\implies \\text{pick the largest eigenvalue } \\lambda _1 \\text{ of covariance matrix } S\\\\\n",
    "    & \\implies u = u_1 \\, \\text{ is the } \\,\\lambda_1's \\,\\text{ corresponding eigenvector}\\\\\n",
    "    & \\implies u_1 \\text{ is the first principal component (direction of highest variance in the data)}\n",
    "    \\end{align*}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463aefef",
   "metadata": {},
   "source": [
    "### 3. Minimize the Sum-of-Squared Error\n",
    "* projection 했을 때 오차가 최소가 되는 단위 벡터 u를 찾는 것은 분산이 최대가 되는 u를 찾는 것과 동일함\n",
    "* Step 1. Derivation of Error Term\n",
    "    * $x^{(i)}$ vector를 단위 벡터 $u$에 projection 한 vector 크기:\n",
    "    $$\n",
    "    \\lVert\\Pr_u(x^{(i)})\\rVert= \\lVert(u^T \\cdot x^{(i)})u\\rVert = u^T \\cdot x^{(i)} = {x^{(i)}}^T \\cdot u \n",
    "    $$\n",
    "    * $x^{(i)}, \\Pr_u(x^{(i)}), e^{(i)}$가 직각삼각형이기 때문에 피타고라스 정리에 의해 : \n",
    "    $$\n",
    "    \\begin{align*} \\lVert e^{(i)} \\rVert ^2 \n",
    "    & = \\lVert x^{(i)} \\rVert ^2 - \\lVert\\Pr_u(x^{(i)})\\rVert^2 \\\\\n",
    "    & = \\lVert x^{(i)} \\rVert ^2 - \\big( {x^{(i)}}^Tu \\big)^2 \\\\\n",
    "    & = \\lVert x^{(i)} \\rVert ^2 - \\big( {x^{(i)}}^Tu \\big)^T\\big( {x^{(i)}}^Tu \\big) \\\\\n",
    "    & = \\lVert x^{(i)} \\rVert ^2 - u^Tx^{(i)}{x^{(i)}}^Tu\\\\\n",
    "    \\end{align*}\n",
    "    $$\n",
    "* Step 2. Minimizing the Mean Squared Error\n",
    "    * X 의 모든 data의 오류제곱을 평균 내면, 즉 MSE를 구하면\n",
    "    * 위 식을 대입하여 식을 정리함\n",
    "    $$\n",
    "    \\begin{align*} \\frac {1}{m} \\sum\\limits_{i=1}^{m} \\lVert e^{(i)} \\rVert ^2\n",
    "    & = \\frac {1}{m} \\sum\\limits_{i=1}^{m} \\lVert x^{(i)} \\rVert ^2 -\n",
    "    \\frac{1}{m}\\sum\\limits_{i=1}^{m}u^Tx^{(i)}{x^{(i)}}^Tu \\\\\n",
    "    & = \\frac {1}{m} \\sum\\limits_{i=1}^{m} \\lVert x^{(i)} \\rVert ^2  -\n",
    "    u^T\\left(\\frac{1}{m}\\sum\\limits_{i=1}^{m}x^{(i)}{x^{(i)}}^T\\right)u \\end{align*}\n",
    "    $$\n",
    "* Step 3. Optimization Formulation\n",
    "    * $ x^{(i)}$는 주어진 데이터에 의해 정해진 상수이기 때문에 MSE를 최소화 하는 것은 두번째 항을 최대화 하는 것과 같음\n",
    "    $$\n",
    "    \\begin{align*} \n",
    "    &\\text{minimize} \\; \\underbrace{\\frac {1}{m} \\sum\\limits_{i=1}^{m} \\lVert x^{(i)} \\rVert ^2}_{\\text{constant given $x_i$}}  -\n",
    "    \\underbrace{u^T\\left(\\frac{1}{m}\\sum\\limits_{i=1}^{m}x^{(i)}{x^{(i)}}^T\\right)u}_{\\text{maximize}} \\\\ \n",
    "    \\\\\n",
    "    \\implies &\\text{maximize} \\;\n",
    "    u^T\\left(\\frac{1}{m}\\sum\\limits_{i=1}^{m}x^{(i)}{x^{(i)}}^T\\right)u = \\max \\; u^T S u\n",
    "    \\end{align*}\n",
    "    $$\n",
    "    * 결국 X를 projection 할 u를 찾는 방식이 달라도 동일한 u를 찾는 것임\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    & = Variance \\text{를 최대로 하는 단위벡터 } u \\\\\n",
    "    & = MSE \\text{를 최소로 하는 단위벡터 } u \\\\\n",
    "    & = u^T S u \\text{를 최대로 하는 단위벡터 } u\n",
    "    \\end{align*}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f10ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bfa4b02",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "boostcamp (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
